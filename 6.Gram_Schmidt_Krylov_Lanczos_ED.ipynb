{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning outcomes: We will learn/review\n",
    "## [A] Gram Schmidt Diagonalisation\n",
    "## [B] Arnoldi\n",
    "## [C] Krylov and Lanczos [not now.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the notation in Wikipedia, so that you can reference it readily https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process\n",
    "\n",
    "## [SG1] Let $proj_{u}(v):=\\frac{\\langle v\\vert u\\rangle}{\\langle u\\vert u\\rangle}\\vert u\\rangle$\n",
    "\n",
    "## [SG2] Given $k$ vectors $\\{\\vert v_i\\rangle\\}$, the Gram-Schmidt algorithm (which you should be studying in Math Phys and PH434 Quantum Mechanics) is given by the following ORTHOGONAL construction of vectors $\\{\\vert u_i\\rangle\\}$.\n",
    "\n",
    "## $\\vert u_1\\rangle=\\vert v_1\\rangle$\n",
    "\n",
    "## $\\vert u_2\\rangle=\\vert v_2\\rangle-proj_{u_1}(\\vert v_2\\rangle)$\n",
    "\n",
    "## $\\vert u_k\\rangle=\\vert v_k\\rangle-\\sum_{j=1}^{k-1}proj_{u_j}(\\vert v_k\\rangle)$\n",
    "\n",
    "## [SG3] You may further normalize $\\{\\vert v_i\\rangle\\}$ to get a ORTHONORMAL set $\\{\\vert e_i\\rangle\\}$. (See https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process#Numerical_stability for modified GS, which has better numerical stability.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Write a code to implement GramSchmidt. It should accept a square matrix, whose first index is the vector index and whose second index stores the corresponding vector (i.e., M[i,:] should be $v_i$) and whose output is a matrix of the same size, but orthonormal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Linear algebra, the order $r-$Krylov subspace generated by a $n\\times n$ matrix and a vector $\\vert b\\rangle$ is given by $span(\\vert b\\rangle,\\vert A b\\rangle,\\vert A^2 b\\rangle,\\ldots \\vert A^{r-1}b\\rangle)$\n",
    "### \"The Arnoldi method\" starts with a Krylov subspace of a pair $\\{A,\\vert b\\rangle\\}$ and constructs a set of $r$ projectors. These projectors can be used to approximate $e^{At}\\vert b\\rangle$ for some time $t$ which is related to $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Write a code to implement Krylov. It should accept $\\{A,\\vert b\\rangle,r\\}$ and return $r$ vectors. Use GramSchmidt above to produce a Arnoldi projection of $e^At\\vert b\\rangle$. \n",
    "\n",
    "## [3] Consider the matrix $H=\\omega_0\\hat{n}+J(\\hat{a}+\\hat{a}^\\dag)$. Here $\\hat{n}=\\sum_{k=0}^{N}k\\vert k\\rangle\\langle k\\vert$ and $\\hat{a}=\\sum_{k=0}^{N}\\sqrt{k}\\vert k-1\\rangle\\langle k\\vert$. Write a code to implement Arnoldi on $e^{-i Ht}\\vert\\psi\\rangle$ where $\\vert\\psi\\rangle=\\frac{\\vert 33\\rangle + \\vert 44\\rangle}{\\sqrt 2}$. Here $\\vert k\\rangle$ has zeros everywhere in a column vector except entry $k$, which is 1. Take $N=10000$ and explore for different times $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lets do Lanczos once this is all a bit more clear.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
